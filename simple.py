# -----------------------------------------
# MODEL A: SIMPLE LINEAR REGRESSION
# -----------------------------------------

# ğŸ“¦ Import required libraries
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ”¹ Step 1: Load the California Housing dataset
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target  # Median house value in $100,000s

# ğŸ”¹ Step 2: Split the dataset into training and testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ğŸ”¹ Step 3: Initialize and train the Linear Regression model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# ğŸ”¹ Step 4: Make predictions on the test data
y_pred = lin_reg.predict(X_test)

# ğŸ”¹ Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("ğŸ“Š Simple Linear Regression Results:")
print("â¡ï¸  RÂ² Score :", round(r2, 4))
print("â¡ï¸  Mean Squared Error (MSE):", round(mse, 4))

# ğŸ”¹ Step 6: Optional - View coefficients
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': lin_reg.coef_
})
print("\nğŸ“Œ Feature Coefficients:\n", coefficients)

# ğŸ”¹ Step 7: Visualize Actual vs Predicted
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)
plt.xlabel("Actual House Prices")
plt.ylabel("Predicted House Prices")
plt.title("Linear Regression: Actual vs Predicted")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect fit line
plt.tight_layout()
plt.show()
